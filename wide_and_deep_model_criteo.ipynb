{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 宽度深度模型/wide and deep model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 介绍\n",
    "\n",
    "在之前的代码里大家看到了如何用tensorflow自带的op来构建灵活的神经网络，这里用tf中的高级接口，用更简单的方式完成wide&deep模型。\n",
    "\n",
    "大家都知道google官方给出的典型wide&deep模型结构如下：\n",
    "![](https://img-blog.csdn.net/20170502135611349)\n",
    "\n",
    "更一般的拼接模型ctr预估结构可以如下：\n",
    "![](https://yxzf.github.io/images/deeplearning/dnn_ctr/embeding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入工具库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version 1.12.0\n",
      "\n",
      "Feature columns are:  ['I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11', 'I12', 'I13', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26'] \n",
      "\n",
      "Columns and data as a dict:  {'C19': 'f6a3e43b', 'C18': 'bd17c3da', 'C13': '7203f04e', 'C12': '79507c6b', 'C11': '77212bd7', 'C10': 'ceb10289', 'C17': '8efede7f', 'C16': '49013ffe', 'C15': '2c14c412', 'C14': '07d13a8f', 'I9': 475, 'I8': 17, 'I1': 0, 'I3': 1, 'I2': 127, 'I5': 1683, 'I4': 3, 'I7': 26, 'I6': 19, 'C9': 'a73ee510', 'C8': '0b153874', 'C3': '11c9d79e', 'C2': '8947f767', 'C1': '05db9164', 'C7': '18671b18', 'C6': 'fbad5c96', 'C5': '4cf72387', 'C4': '52a787c8', 'C22': 'ad3062eb', 'C23': 'c7dc6720', 'C20': 'a458ea53', 'C21': '35cd95c9', 'C26': '49d68486', 'C24': '3fdb382b', 'C25': '010f6491', 'I11': 9, 'I10': 0, 'I13': 3, 'I12': 0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(\"Using TensorFlow version %s\\n\" % (tf.__version__))\n",
    "\n",
    "# 我们这里使用的是criteo数据集，X的部分包括13个连续值列和26个类别型值的列\n",
    "CONTINUOUS_COLUMNS =  [\"I\"+str(i) for i in range(1,14)] # 1-13 inclusive\n",
    "CATEGORICAL_COLUMNS = [\"C\"+str(i) for i in range(1,27)] # 1-26 inclusive\n",
    "# 标签是clicked\n",
    "LABEL_COLUMN = [\"clicked\"]\n",
    "\n",
    "# 训练集由 label列 + 连续值列 + 离散值列 构成\n",
    "TRAIN_DATA_COLUMNS = LABEL_COLUMN + CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "#TEST_DATA_COLUMNS = CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "\n",
    "# 特征列就是 连续值列+离散值列\n",
    "FEATURE_COLUMNS = CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "\n",
    "# 输出一些信息\n",
    "print('Feature columns are: ', FEATURE_COLUMNS, '\\n')\n",
    "\n",
    "# 数据示例\n",
    "sample = [ 0, 127, 1, 3, 1683, 19, 26, 17, 475, 0, 9, 0, 3, \"05db9164\", \"8947f767\", \"11c9d79e\", \"52a787c8\", \"4cf72387\", \"fbad5c96\", \"18671b18\", \"0b153874\", \"a73ee510\", \"ceb10289\", \"77212bd7\", \"79507c6b\", \"7203f04e\", \"07d13a8f\", \"2c14c412\", \"49013ffe\", \"8efede7f\", \"bd17c3da\", \"f6a3e43b\", \"a458ea53\", \"35cd95c9\", \"ad3062eb\", \"c7dc6720\", \"3fdb382b\", \"010f6491\", \"49d68486\"]\n",
    "\n",
    "print('Columns and data as a dict: ', dict(zip(FEATURE_COLUMNS, sample)), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入文件解析\n",
    "\n",
    "我们把数据送进`Reader`然后从文件里一次读一个batch \n",
    "\n",
    "对`_input_fn()`函数做了特殊的封装处理，使得它更适合不同类型的文件读取\n",
    "\n",
    "注意一下：这里的文件是直接通过tensorflow读取的，我们没有用pandas这种工具，也没有一次性把所有数据读入内存，这样对于非常大规模的数据文件训练，是合理的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于input_fn函数\n",
    "\n",
    "这个函数定义了我们怎么读取数据用于训练和测试。这里的返回结果是一个pair对，第一个元素是列名到具体取值的映射字典，第二个元素是label的序列。\n",
    "\n",
    "抽象一下，大概是这么个东西 `map(column_name => [Tensor of values]) , [Tensor of labels])`\n",
    "\n",
    "举个例子就长这样：\n",
    "\n",
    "    { \n",
    "      'age':            [ 39, 50, 38, 53, 28, … ], \n",
    "      'marital_status': [ 'Married-civ-spouse', 'Never-married', 'Widowed', 'Widowed' … ],\n",
    "       ...\n",
    "      'gender':           ['Male', 'Female', 'Male', 'Male', 'Female',, … ], \n",
    "    } , \n",
    "    [ 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level structure of input functions for CSV-style data\n",
    "1. Queue file(s)\n",
    "2. Read a batch of data from the next file\n",
    "3. Create record defaults, generally 0 for continuous values, and \"\" for categorical. You can use named types if you prefer\n",
    "4. Decode the CSV and restructure it to be appropriate for the graph's input format\n",
    "    * `zip()` column headers with the data\n",
    "    * `pop()` off the label column(s)\n",
    "    * Remove/pop any unneeded column(s)\n",
    "    * Run `tf.expand_dims()` on categorical columns\n",
    "    5. Return the pair: `(feature_dict, label_array)`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input function configured\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2000\n",
    "\n",
    "def generate_input_fn(filename, batch_size=BATCH_SIZE):\n",
    "    def _input_fn():\n",
    "        filename_queue = tf.train.string_input_producer([filename])\n",
    "        reader = tf.TextLineReader()\n",
    "        # 只读batch_size行\n",
    "        key, value = reader.read_up_to(filename_queue, num_records=batch_size)\n",
    "        \n",
    "        # 1个int型的label, 13个连续值, 26个字符串类型\n",
    "        cont_defaults = [ [0] for i in range(1,14) ]\n",
    "        cate_defaults = [ [\" \"] for i in range(1,27) ]\n",
    "        label_defaults = [ [0] ]\n",
    "        column_headers = TRAIN_DATA_COLUMNS\n",
    "        \n",
    "        # 第一列数据是label\n",
    "        record_defaults = label_defaults + cont_defaults + cate_defaults\n",
    "\n",
    "        # 解析读出的csv数据\n",
    "        # 我们要手动把数据和header去zip在一起\n",
    "        columns = tf.decode_csv(\n",
    "            value, record_defaults=record_defaults, field_delim='\\t')\n",
    "        \n",
    "        # 最终是列名到数据张量的映射字典\n",
    "        all_columns = dict(zip(column_headers, columns))\n",
    "        \n",
    "        # 弹出和保存label标签\n",
    "        labels = all_columns.pop(LABEL_COLUMN[0])\n",
    "        \n",
    "        # 其余列就是特征\n",
    "        features = all_columns \n",
    "\n",
    "        # 类别型的列我们要做一个类似one-hot的扩展操作\n",
    "        for feature_name in CATEGORICAL_COLUMNS:\n",
    "            features[feature_name] = tf.expand_dims(features[feature_name], -1)\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "    return _input_fn\n",
    "\n",
    "print('input function configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建特征列\n",
    "这个部分我们来看一下用tensorflow的高级接口，如何方便地对特征进行处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 稀疏列/Sparse Columns\n",
    "我们先构建稀疏列(针对类别型)\n",
    "\n",
    "对于所有类别取值都清楚的我们用`sparse_column_with_keys()`处理\n",
    "\n",
    "对于类别可能比较多，没办法枚举的可以试试用`sparse_column_with_hash_bucket()`处理这个映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide/Sparse columns configured\n"
     ]
    }
   ],
   "source": [
    "# Sparse base columns.\n",
    "# C1 = tf.contrib.layers.sparse_column_with_hash_bucket('C1', hash_bucket_size=1000)\n",
    "# C2 = tf.contrib.layers.sparse_column_with_hash_bucket('C2', hash_bucket_size=1000)\n",
    "# C3 = tf.contrib.layers.sparse_column_with_hash_bucket('C3', hash_bucket_size=1000)\n",
    "# ...\n",
    "# Cn = tf.contrib.layers.sparse_column_with_hash_bucket('Cn', hash_bucket_size=1000)\n",
    "# wide_columns = [C1, C2, C3, ... , Cn]\n",
    "\n",
    "wide_columns = []\n",
    "for name in CATEGORICAL_COLUMNS:\n",
    "    wide_columns.append(tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "            name, hash_bucket_size=1000))\n",
    "\n",
    "print('Wide/Sparse columns configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 连续值列/Continuous columns\n",
    "通过`real_valued_column()`设定连续值列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep/continuous columns configured\n"
     ]
    }
   ],
   "source": [
    "# Continuous base columns.\n",
    "# I1 = tf.contrib.layers.real_valued_column(\"I1\")\n",
    "# I2 = tf.contrib.layers.real_valued_column(\"I2\")\n",
    "# I3 = tf.contrib.layers.real_valued_column(\"I3\")\n",
    "# ...\n",
    "# In = tf.contrib.layers.real_valued_column(\"In\")\n",
    "# deep_columns = [I1, I2, I3, ... , In]\n",
    "\n",
    "deep_columns = []\n",
    "for name in CONTINUOUS_COLUMNS:\n",
    "    deep_columns.append(tf.contrib.layers.real_valued_column(name))\n",
    "\n",
    "print('deep/continuous columns configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征工程变换\n",
    "因为这是一份做过脱敏处理的数据，所以我们做下面的2个操作\n",
    " \n",
    "* **分桶/bucketizing** 对连续值离散化和分桶\n",
    "* **生成交叉特征/feature crossing** 对2列或者多列去构建交叉组合特征(注意只有离散的特征才能交叉，所以如果连续值特征要用这个处理，要先离散化) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations complete\n"
     ]
    }
   ],
   "source": [
    "# No known Transformations. Can add some if desired. \n",
    "# Examples from other datasets are shown below.\n",
    "\n",
    "# age_buckets = tf.contrib.layers.bucketized_column(age,\n",
    "#             boundaries=[ 18, 25, 30, 35, 40, 45, 50, 55, 60, 65 ])\n",
    "# education_occupation = tf.contrib.layers.crossed_column([education, occupation], \n",
    "#                                                         hash_bucket_size=int(1e4))\n",
    "# age_race_occupation = tf.contrib.layers.crossed_column([age_buckets, race, occupation], \n",
    "#                                                        hash_bucket_size=int(1e6))\n",
    "# country_occupation = tf.contrib.layers.crossed_column([native_country, occupation], \n",
    "#                                                       hash_bucket_size=int(1e4))\n",
    "\n",
    "print('Transformations complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group feature columns into 2 objects\n",
    "\n",
    "The wide columns are the sparse, categorical columns that we specified, as well as our hashed, bucket, and feature crossed columns. \n",
    "\n",
    "The deep columns are composed of embedded categorical columns along with the continuous real-valued columns. **Column embeddings** transform a sparse, categorical tensor into a low-dimensional and dense real-valued vector. The embedding values are also trained along with the rest of the model. For more information about embeddings, see the TensorFlow tutorial on [Vector Representations Words](https://www.tensorflow.org/tutorials/word2vec/), or [Word Embedding](https://en.wikipedia.org/wiki/Word_embedding) on Wikipedia.\n",
    "\n",
    "The higher the dimension of the embedding is, the more degrees of freedom the model will have to learn the representations of the features. We are starting with an 8-dimension embedding for simplicity, but later you can come back and increase the dimensionality if you wish.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "wide and deep columns configured\n"
     ]
    }
   ],
   "source": [
    "# Wide columns and deep columns.\n",
    "# wide_columns = [gender, race, native_country,\n",
    "#       education, occupation, workclass,\n",
    "#       marital_status, relationship,\n",
    "#       age_buckets, education_occupation,\n",
    "#       age_race_occupation, country_occupation]\n",
    "\n",
    "# deep_columns = [\n",
    "#   tf.contrib.layers.embedding_column(workclass, dimension=8),\n",
    "#   tf.contrib.layers.embedding_column(education, dimension=8),\n",
    "#   tf.contrib.layers.embedding_column(marital_status, dimension=8),\n",
    "#   tf.contrib.layers.embedding_column(gender, dimension=8),\n",
    "#   tf.contrib.layers.embedding_column(relationship, dimension=8),\n",
    "#   tf.contrib.layers.embedding_column(race, dimension=8),\n",
    "#   tf.contrib.layers.embedding_column(native_country, dimension=8),\n",
    "#   tf.contrib.layers.embedding_column(occupation, dimension=8),\n",
    "#   age,\n",
    "#   education_num,\n",
    "#   capital_gain,\n",
    "#   capital_loss,\n",
    "#   hours_per_week,\n",
    "# ]\n",
    "\n",
    "# Embeddings for wide columns into deep columns\n",
    "for col in wide_columns:\n",
    "    deep_columns.append(tf.contrib.layers.embedding_column(col, \n",
    "                                                           dimension=8))\n",
    "\n",
    "print('wide and deep columns configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型\n",
    "\n",
    "你可以根据实际情况构建“宽模型”、“深模型”、“深度宽度模型”\n",
    "\n",
    "* **Wide**: 相当于逻辑回归\n",
    "* **Deep**: 相当于多层感知器\n",
    "* **Wide & Deep**: 组合两种结构\n",
    "\n",
    "这里有2个参数`hidden_units` 或者 `dnn_hidden_units`可以指定隐层的节点个数，比如`[12, 20, 15]`构建3层神经元个数分别为12、20、15的隐层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory = ./models/model_WIDE_AND_DEEP_1551328373\n",
      "WARNING:tensorflow:From <ipython-input-7-b45104c07846>:37: calling __init__ (from tensorflow.contrib.learn.python.learn.estimators.dnn_linear_combined) with fix_global_step_increment_bug=False is deprecated and will be removed after 2017-04-15.\n",
      "Instructions for updating:\n",
      "Please set fix_global_step_increment_bug=True and update training steps in your pipeline. See pydoc for details.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py:676: multi_class_head (from tensorflow.contrib.learn.python.learn.estimators.head) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.contrib.estimator.*_head.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1180: __init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please replace uses of any Estimator from tf.contrib.learn with an Estimator from tf.estimator.*\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2f31b0f9d0>, '_model_dir': './models/model_WIDE_AND_DEEP_1551328373', '_protocol': None, '_save_checkpoints_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}\n",
      "estimator built\n"
     ]
    }
   ],
   "source": [
    "def create_model_dir(model_type):\n",
    "    # 返回类似这样的结果 models/model_WIDE_AND_DEEP_1493043407\n",
    "    return './models/model_' + model_type + '_' + str(int(time.time()))\n",
    "\n",
    "# 指定模型文件夹\n",
    "def get_model(model_type, model_dir):\n",
    "    print(\"Model directory = %s\" % model_dir)\n",
    "    \n",
    "    # 对checkpoint去做设定\n",
    "    runconfig = tf.estimator.RunConfig(\n",
    "        save_checkpoints_secs=None,\n",
    "        save_checkpoints_steps = 100,\n",
    "    )\n",
    "    \n",
    "    m = None\n",
    "    \n",
    "    # 宽模型\n",
    "    if model_type == 'WIDE':\n",
    "        m = tf.estimator.LinearClassifier(\n",
    "            model_dir=model_dir, \n",
    "            feature_columns=wide_columns)\n",
    "\n",
    "    # 深度模型\n",
    "    if model_type == 'DEEP':\n",
    "        m = tf.estimator.DNNClassifier(\n",
    "            model_dir=model_dir,\n",
    "            feature_columns=deep_columns,\n",
    "            hidden_units=[100, 50, 25])\n",
    "\n",
    "    # 宽度深度模型\n",
    "    if model_type == 'WIDE_AND_DEEP':\n",
    "        m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "            model_dir=model_dir,\n",
    "            linear_feature_columns=wide_columns,\n",
    "            dnn_feature_columns=deep_columns,\n",
    "            dnn_hidden_units=[100, 70, 50, 25],\n",
    "            config=runconfig)\n",
    "        \n",
    "    print('estimator built')\n",
    "    \n",
    "    return m\n",
    "    \n",
    "\n",
    "MODEL_TYPE = 'WIDE_AND_DEEP'\n",
    "model_dir = create_model_dir(model_type=MODEL_TYPE)\n",
    "m = get_model(model_type=MODEL_TYPE, model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评估\n",
    "from tensorflow.contrib.learn.python.learn import evaluable, trainable\n",
    "isinstance(m, evaluable.Evaluable)\n",
    "isinstance(m, trainable.Trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拟合与模型训练\n",
    "\n",
    "执行`fit()`函数训练模型，可以试试不同的`train_steps`和`BATCH_SIZE`参数，会影响速度和结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,4,14,7,28,36,28,4,43,47,2,2,0,28,05db9164,39dfaa0d,dd17c91c,82a61820,25c83c98,fe6b92e5,8f99333a,5b392875,a73ee510,3b08e48b,3ad41aaa,75529ad8,4ca13ee8,07d13a8f,60fa10e5,5eea53aa,e5ba7672,df4fffb7,21ddcdc9,5840adea,0f78ab39,,32c7478e,cafb4e4d,010f6491,99f4f64c\r\n",
      "1,,0,11,,87896,,,7,,,,,,05db9164,0a519c5c,ad4b77ff,d16679b9,25c83c98,7e0ccccf,fa44c4cf,0b153874,7cc72ec2,30fdb872,2b9f131d,a2f4e8b5,aca10c14,07d13a8f,5a7d5bd8,89052618,d4bb7bd8,eea3ab97,,,d4703ebd,,3a171ecb,aee52b6f,,\r\n",
      "0,2,2,28,7,1,1,21,7,393,1,4,0,1,05db9164,bccb7a1a,3cc14b5b,8e9c10ae,25c83c98,7e0ccccf,36b21dc8,51d76abe,a73ee510,451bd4e4,0f1fa8b8,8527be14,e4e9ce3a,b28479f6,1302f720,3aaae0a8,07c540c4,d51975d7,21ddcdc9,5840adea,6a4bdd9b,,3a171ecb,340d03c3,e8b83407,96911ece\r\n",
      "1,,-1,4,25,32258,111,2,36,80,,1,0,25,05db9164,8084ee93,d032c263,c18be181,25c83c98,fbad5c96,3fbde16c,0b153874,a73ee510,83ff688a,087dfcfd,dfbb09fb,5317f239,07d13a8f,422c8577,84898b2a,d4bb7bd8,52e44668,,,0014c32a,,3a171ecb,3b183c5c,,\r\n",
      "0,0,0,28,7,1457,29,5,14,232,0,3,,7,68fd1e64,80e26c9b,49faa80a,119cf591,4cf72387,7e0ccccf,cc5ed2f1,64523cfa,a73ee510,3b08e48b,a95a8954,45be498e,9f16a973,07d13a8f,f3635baf,25bae044,07c540c4,f54016b9,21ddcdc9,b1252a9d,074e67fe,,32c7478e,e3c310aa,e8b83407,606ab942\r\n"
     ]
    }
   ],
   "source": [
    "# 训练文件与测试文件\n",
    "train_file = \"./data/criteo/criteo_data/train.txt\"\n",
    "eval_file  = \"./data/criteo/criteo_data/train.txt\"\n",
    "!head -5 ./data/criteo/criteo_data/criteo_train.txt\n",
    "\n",
    "test_file= \"./data/criteo/criteo_data/test.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-259a1c630309>:5: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/python/training/input.py:197: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-2-259a1c630309>:6: __init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TextLineDataset`.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Casting <dtype: 'int32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'int32'> labels to bool.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:678: __new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:loss = 441.396, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 102 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 13.414\n",
      "INFO:tensorflow:loss = 1.30068, step = 200 (12.946 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 204 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 16.3895\n",
      "INFO:tensorflow:Saving checkpoints for 306 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 24.4147\n",
      "INFO:tensorflow:loss = 0.537378, step = 400 (8.177 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 408 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.5645\n",
      "INFO:tensorflow:Saving checkpoints for 510 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 24.0098\n",
      "INFO:tensorflow:loss = 0.569089, step = 600 (8.069 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 612 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.7836\n",
      "INFO:tensorflow:Saving checkpoints for 714 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.3153\n",
      "INFO:tensorflow:loss = 0.627646, step = 800 (7.984 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 816 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 23.9981\n",
      "INFO:tensorflow:Saving checkpoints for 918 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.5833\n",
      "INFO:tensorflow:loss = 0.505549, step = 1000 (8.259 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1020 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 24.2947\n",
      "INFO:tensorflow:Saving checkpoints for 1122 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 24.3196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.500255, step = 1200 (8.425 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1224 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 23.7949\n",
      "INFO:tensorflow:Saving checkpoints for 1326 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.3801\n",
      "INFO:tensorflow:loss = 0.494852, step = 1400 (7.396 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1428 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.0794\n",
      "INFO:tensorflow:Saving checkpoints for 1530 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.7893\n",
      "INFO:tensorflow:loss = 0.495444, step = 1600 (7.787 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1632 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 24.66\n",
      "INFO:tensorflow:Saving checkpoints for 1734 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.0025\n",
      "INFO:tensorflow:loss = 0.520328, step = 1800 (7.980 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1836 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.7785\n",
      "INFO:tensorflow:Saving checkpoints for 1938 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.2458\n",
      "INFO:tensorflow:loss = 0.520117, step = 2000 (7.792 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2040 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 24.395\n",
      "INFO:tensorflow:Saving checkpoints for 2142 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.2272\n",
      "INFO:tensorflow:loss = 0.498432, step = 2200 (7.619 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2244 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.6718\n",
      "INFO:tensorflow:Saving checkpoints for 2346 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.9876\n",
      "INFO:tensorflow:loss = 0.491736, step = 2400 (7.549 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2448 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.0735\n",
      "INFO:tensorflow:Saving checkpoints for 2550 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.1622\n",
      "INFO:tensorflow:loss = 0.486067, step = 2600 (7.475 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2652 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.7319\n",
      "INFO:tensorflow:Saving checkpoints for 2754 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.891\n",
      "INFO:tensorflow:loss = 0.516576, step = 2800 (7.161 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2856 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.4374\n",
      "INFO:tensorflow:Saving checkpoints for 2958 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.3136\n",
      "INFO:tensorflow:loss = 0.492, step = 3000 (7.501 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3060 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.7453\n",
      "INFO:tensorflow:Saving checkpoints for 3162 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.7636\n",
      "INFO:tensorflow:loss = 0.486412, step = 3200 (7.529 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3264 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.4959\n",
      "INFO:tensorflow:Saving checkpoints for 3366 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.6186\n",
      "INFO:tensorflow:loss = 0.487244, step = 3400 (7.300 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3468 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.722\n",
      "INFO:tensorflow:Saving checkpoints for 3570 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.1217\n",
      "INFO:tensorflow:loss = 0.488274, step = 3600 (7.550 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3672 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.7575\n",
      "INFO:tensorflow:Saving checkpoints for 3774 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.7702\n",
      "INFO:tensorflow:loss = 0.511511, step = 3800 (7.521 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3876 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.4617\n",
      "INFO:tensorflow:Saving checkpoints for 3978 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.7193\n",
      "INFO:tensorflow:loss = 0.513202, step = 4000 (7.191 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4080 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.7843\n",
      "INFO:tensorflow:Saving checkpoints for 4182 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.9408\n",
      "INFO:tensorflow:loss = 0.490396, step = 4200 (7.578 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4284 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.0956\n",
      "INFO:tensorflow:Saving checkpoints for 4386 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.7942\n",
      "INFO:tensorflow:loss = 0.48514, step = 4400 (7.465 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4488 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.3822\n",
      "INFO:tensorflow:Saving checkpoints for 4590 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.1321\n",
      "INFO:tensorflow:loss = 0.471648, step = 4600 (7.503 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4692 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.8513\n",
      "INFO:tensorflow:Saving checkpoints for 4794 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.607\n",
      "INFO:tensorflow:loss = 0.504798, step = 4800 (7.218 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4896 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.024\n",
      "INFO:tensorflow:Saving checkpoints for 4998 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.2145\n",
      "INFO:tensorflow:loss = 0.491582, step = 5000 (7.556 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5100 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.6804\n",
      "INFO:tensorflow:Saving checkpoints for 5202 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.482246, step = 5200 (7.566 sec)\n",
      "INFO:tensorflow:global_step/sec: 26.043\n",
      "INFO:tensorflow:Saving checkpoints for 5304 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.6334\n",
      "INFO:tensorflow:loss = 0.482367, step = 5400 (6.775 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5406 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.1899\n",
      "INFO:tensorflow:Saving checkpoints for 5508 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.666\n",
      "INFO:tensorflow:loss = 0.483759, step = 5600 (7.617 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5610 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.0328\n",
      "INFO:tensorflow:Saving checkpoints for 5712 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.6359\n",
      "INFO:tensorflow:loss = 0.522666, step = 5800 (7.252 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5814 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.628\n",
      "INFO:tensorflow:Saving checkpoints for 5916 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.7212\n",
      "INFO:tensorflow:loss = 0.510733, step = 6000 (7.473 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6018 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.9468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 6120 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.0768\n",
      "INFO:tensorflow:loss = 0.489546, step = 6200 (7.537 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6222 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.1431\n",
      "INFO:tensorflow:Saving checkpoints for 6324 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.8287\n",
      "INFO:tensorflow:loss = 0.483443, step = 6400 (7.322 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6426 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.9824\n",
      "INFO:tensorflow:Saving checkpoints for 6528 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.351\n",
      "INFO:tensorflow:loss = 0.469083, step = 6600 (7.493 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6630 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.6183\n",
      "INFO:tensorflow:Saving checkpoints for 6732 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.467\n",
      "INFO:tensorflow:loss = 0.500028, step = 6800 (7.408 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6834 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.487\n",
      "INFO:tensorflow:Saving checkpoints for 6936 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.6982\n",
      "INFO:tensorflow:loss = 0.485552, step = 7000 (7.298 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7038 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.7684\n",
      "INFO:tensorflow:Saving checkpoints for 7140 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.8559\n",
      "INFO:tensorflow:loss = 0.478711, step = 7200 (7.200 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7242 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.8307\n",
      "INFO:tensorflow:Saving checkpoints for 7344 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.4463\n",
      "INFO:tensorflow:loss = 0.47715, step = 7400 (7.759 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7446 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.475\n",
      "INFO:tensorflow:Saving checkpoints for 7548 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.6088\n",
      "INFO:tensorflow:loss = 0.481074, step = 7600 (7.378 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7650 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.2757\n",
      "INFO:tensorflow:Saving checkpoints for 7752 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.332\n",
      "INFO:tensorflow:loss = 0.504132, step = 7800 (7.334 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7854 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.5427\n",
      "INFO:tensorflow:Saving checkpoints for 7956 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.4575\n",
      "INFO:tensorflow:loss = 0.523059, step = 8000 (7.275 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8058 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.7106\n",
      "INFO:tensorflow:Saving checkpoints for 8160 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.9426\n",
      "INFO:tensorflow:loss = 0.485035, step = 8200 (7.547 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8262 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.7162\n",
      "INFO:tensorflow:Saving checkpoints for 8364 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.4883\n",
      "INFO:tensorflow:loss = 0.481941, step = 8400 (7.252 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8466 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.0924\n",
      "INFO:tensorflow:Saving checkpoints for 8568 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.6076\n",
      "INFO:tensorflow:loss = 0.472784, step = 8600 (7.479 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8670 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.6989\n",
      "INFO:tensorflow:Saving checkpoints for 8772 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.6514\n",
      "INFO:tensorflow:loss = 0.496645, step = 8800 (7.561 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8874 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.9373\n",
      "INFO:tensorflow:Saving checkpoints for 8976 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.1029\n",
      "INFO:tensorflow:loss = 0.483957, step = 9000 (7.255 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9078 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.1941\n",
      "INFO:tensorflow:Saving checkpoints for 9180 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.9042\n",
      "INFO:tensorflow:loss = 0.475519, step = 9200 (7.436 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9282 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.8652\n",
      "INFO:tensorflow:Saving checkpoints for 9384 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.1169\n",
      "INFO:tensorflow:loss = 0.474453, step = 9400 (7.441 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9486 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.5763\n",
      "INFO:tensorflow:Saving checkpoints for 9588 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.2816\n",
      "INFO:tensorflow:loss = 0.479173, step = 9600 (7.171 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9690 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.0104\n",
      "INFO:tensorflow:Saving checkpoints for 9792 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.8665\n",
      "INFO:tensorflow:loss = 0.509675, step = 9800 (7.337 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9894 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.5827\n",
      "INFO:tensorflow:Saving checkpoints for 9996 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.5141\n",
      "INFO:tensorflow:loss = 0.508553, step = 10000 (7.407 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10098 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.8729\n",
      "INFO:tensorflow:Saving checkpoints for 10200 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.1637\n",
      "INFO:tensorflow:loss = 0.485043, step = 10200 (7.427 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10302 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.1911\n",
      "INFO:tensorflow:loss = 0.476608, step = 10400 (6.762 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10404 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.4571\n",
      "INFO:tensorflow:Saving checkpoints for 10506 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.1913\n",
      "INFO:tensorflow:loss = 0.465027, step = 10600 (7.580 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10608 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.01\n",
      "INFO:tensorflow:Saving checkpoints for 10710 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.2099\n",
      "INFO:tensorflow:loss = 0.496055, step = 10800 (7.334 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10812 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.2018\n",
      "INFO:tensorflow:Saving checkpoints for 10914 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.7984\n",
      "INFO:tensorflow:loss = 0.484294, step = 11000 (7.631 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 11016 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.2331\n",
      "INFO:tensorflow:Saving checkpoints for 11118 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.2125\n",
      "INFO:tensorflow:loss = 0.473511, step = 11200 (7.439 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 11220 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.4216\n",
      "INFO:tensorflow:Saving checkpoints for 11322 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.3704\n",
      "INFO:tensorflow:loss = 0.47522, step = 11400 (7.293 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 11424 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.2469\n",
      "INFO:tensorflow:Saving checkpoints for 11526 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.0522\n",
      "INFO:tensorflow:loss = 0.476592, step = 11600 (7.548 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 11628 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.7439\n",
      "INFO:tensorflow:Saving checkpoints for 11730 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.6855\n",
      "INFO:tensorflow:loss = 0.500644, step = 11800 (7.412 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 11832 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.8652\n",
      "INFO:tensorflow:Saving checkpoints for 11934 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.4945\n",
      "INFO:tensorflow:loss = 0.515399, step = 12000 (7.211 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12036 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.9249\n",
      "INFO:tensorflow:Saving checkpoints for 12138 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.5658\n",
      "INFO:tensorflow:loss = 0.484072, step = 12200 (7.505 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12240 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.7327\n",
      "INFO:tensorflow:Saving checkpoints for 12342 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.7568\n",
      "INFO:tensorflow:loss = 0.474438, step = 12400 (7.566 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12444 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.7349\n",
      "INFO:tensorflow:Saving checkpoints for 12546 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.4119\n",
      "INFO:tensorflow:loss = 0.463333, step = 12600 (7.443 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12648 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.2325\n",
      "INFO:tensorflow:Saving checkpoints for 12750 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.5999\n",
      "INFO:tensorflow:loss = 0.512127, step = 12800 (7.255 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12852 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.9416\n",
      "INFO:tensorflow:Saving checkpoints for 12954 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.1012\n",
      "INFO:tensorflow:loss = 0.501436, step = 13000 (7.618 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13056 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.7704\n",
      "INFO:tensorflow:Saving checkpoints for 13158 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.8285\n",
      "INFO:tensorflow:loss = 0.480011, step = 13200 (7.355 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13260 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.6117\n",
      "INFO:tensorflow:Saving checkpoints for 13362 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.6555\n",
      "INFO:tensorflow:loss = 0.57015, step = 13400 (7.262 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13464 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.117\n",
      "INFO:tensorflow:Saving checkpoints for 13566 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.5569\n",
      "INFO:tensorflow:loss = 0.487325, step = 13600 (7.470 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13668 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.7685\n",
      "INFO:tensorflow:Saving checkpoints for 13770 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.2444\n",
      "INFO:tensorflow:loss = 0.50672, step = 13800 (7.438 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13872 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.8609\n",
      "INFO:tensorflow:Saving checkpoints for 13974 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.8406\n",
      "INFO:tensorflow:loss = 0.505782, step = 14000 (7.181 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 14076 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.2038\n",
      "INFO:tensorflow:Saving checkpoints for 14178 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.1709\n",
      "INFO:tensorflow:loss = 0.48379, step = 14200 (7.561 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 14280 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.43\n",
      "INFO:tensorflow:Saving checkpoints for 14382 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.1403\n",
      "INFO:tensorflow:loss = 0.478371, step = 14400 (7.678 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 14484 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.9225\n",
      "INFO:tensorflow:Saving checkpoints for 14586 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.017\n",
      "INFO:tensorflow:loss = 0.476944, step = 14600 (7.429 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 14688 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.682\n",
      "INFO:tensorflow:Saving checkpoints for 14790 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.8853\n",
      "INFO:tensorflow:loss = 0.508444, step = 14800 (7.492 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 14892 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.7989\n",
      "INFO:tensorflow:Saving checkpoints for 14994 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.5584\n",
      "INFO:tensorflow:loss = 0.48203, step = 15000 (7.661 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 15096 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.2401\n",
      "INFO:tensorflow:Saving checkpoints for 15198 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.8353\n",
      "INFO:tensorflow:loss = 0.476987, step = 15200 (7.294 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 15300 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.956\n",
      "INFO:tensorflow:Saving checkpoints for 15402 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.473735, step = 15400 (7.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 27.9293\n",
      "INFO:tensorflow:Saving checkpoints for 15504 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.2882\n",
      "INFO:tensorflow:loss = 0.478932, step = 15600 (6.901 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 15606 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.8836\n",
      "INFO:tensorflow:Saving checkpoints for 15708 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.6741\n",
      "INFO:tensorflow:loss = 0.509768, step = 15800 (7.250 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 15810 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 25.953\n",
      "INFO:tensorflow:Saving checkpoints for 15912 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.1723\n",
      "INFO:tensorflow:loss = 0.506171, step = 16000 (7.508 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 16014 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.8632\n",
      "INFO:tensorflow:Saving checkpoints for 16116 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.1423\n",
      "INFO:tensorflow:loss = 0.486923, step = 16200 (7.669 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 16218 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.2721\n",
      "INFO:tensorflow:Saving checkpoints for 16320 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.6035\n",
      "INFO:tensorflow:loss = 0.479029, step = 16400 (7.295 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 16422 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.0909\n",
      "INFO:tensorflow:Saving checkpoints for 16524 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.4316\n",
      "INFO:tensorflow:loss = 0.482268, step = 16600 (7.493 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 16626 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.4278\n",
      "INFO:tensorflow:Saving checkpoints for 16728 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.7888\n",
      "INFO:tensorflow:loss = 0.510332, step = 16800 (7.530 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 16830 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.3392\n",
      "INFO:tensorflow:Saving checkpoints for 16932 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.8331\n",
      "INFO:tensorflow:loss = 0.484012, step = 17000 (7.428 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 17034 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.9834\n",
      "INFO:tensorflow:Saving checkpoints for 17136 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.1989\n",
      "INFO:tensorflow:loss = 0.474324, step = 17200 (7.570 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 17238 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.4461\n",
      "INFO:tensorflow:Saving checkpoints for 17340 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.5471\n",
      "INFO:tensorflow:loss = 0.474976, step = 17400 (7.493 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 17442 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.1305\n",
      "INFO:tensorflow:Saving checkpoints for 17544 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.2572\n",
      "INFO:tensorflow:loss = 0.478287, step = 17600 (7.323 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 17646 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.3136\n",
      "INFO:tensorflow:Saving checkpoints for 17748 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.976\n",
      "INFO:tensorflow:loss = 0.521693, step = 17800 (7.398 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 17850 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.295\n",
      "INFO:tensorflow:Saving checkpoints for 17952 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.6827\n",
      "INFO:tensorflow:loss = 0.507427, step = 18000 (7.453 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 18054 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.9323\n",
      "INFO:tensorflow:Saving checkpoints for 18156 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.722\n",
      "INFO:tensorflow:loss = 0.488965, step = 18200 (7.325 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 18258 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.6881\n",
      "INFO:tensorflow:Saving checkpoints for 18360 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.0331\n",
      "INFO:tensorflow:loss = 0.478161, step = 18400 (7.209 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 18462 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.9455\n",
      "INFO:tensorflow:Saving checkpoints for 18564 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.3792\n",
      "INFO:tensorflow:loss = 0.487117, step = 18600 (7.570 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 18666 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.1694\n",
      "INFO:tensorflow:Saving checkpoints for 18768 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.789\n",
      "INFO:tensorflow:loss = 0.51729, step = 18800 (7.549 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 18870 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.5165\n",
      "INFO:tensorflow:Saving checkpoints for 18972 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.9507\n",
      "INFO:tensorflow:loss = 0.481632, step = 19000 (7.259 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 19074 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.6823\n",
      "INFO:tensorflow:Saving checkpoints for 19176 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.4802\n",
      "INFO:tensorflow:loss = 0.470507, step = 19200 (7.568 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 19278 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.9273\n",
      "INFO:tensorflow:Saving checkpoints for 19380 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.8658\n",
      "INFO:tensorflow:loss = 0.473833, step = 19400 (7.486 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 19482 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.336\n",
      "INFO:tensorflow:Saving checkpoints for 19584 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.6634\n",
      "INFO:tensorflow:loss = 0.474647, step = 19600 (7.254 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 19686 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.5545\n",
      "INFO:tensorflow:Saving checkpoints for 19788 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.8083\n",
      "INFO:tensorflow:loss = 0.498921, step = 19800 (7.389 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 19890 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.1669\n",
      "INFO:tensorflow:Saving checkpoints for 19992 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.3307\n",
      "INFO:tensorflow:loss = 0.504697, step = 20000 (7.506 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 20002 into ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.504697.\n",
      "fit done\n"
     ]
    }
   ],
   "source": [
    "# This can be found with\n",
    "# wc -l train.csv\n",
    "train_sample_size = 2000000\n",
    "train_steps = train_sample_size/BATCH_SIZE*20\n",
    "\n",
    "m.fit(input_fn=generate_input_fn(train_file, BATCH_SIZE), steps=train_steps)\n",
    "\n",
    "print('fit done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估模型准确率\n",
    "评估准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Casting <dtype: 'int32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'int32'> labels to bool.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-28-04:46:02\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./models/model_WIDE_AND_DEEP_1551328373/model.ckpt-20002\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [25/250]\n",
      "INFO:tensorflow:Evaluation [50/250]\n",
      "INFO:tensorflow:Evaluation [75/250]\n",
      "INFO:tensorflow:Evaluation [100/250]\n",
      "INFO:tensorflow:Evaluation [125/250]\n",
      "INFO:tensorflow:Evaluation [150/250]\n",
      "INFO:tensorflow:Evaluation [175/250]\n",
      "INFO:tensorflow:Evaluation [200/250]\n",
      "INFO:tensorflow:Evaluation [225/250]\n",
      "INFO:tensorflow:Evaluation [250/250]\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-28-04:46:16\n",
      "INFO:tensorflow:Saving dict for global step 20002: accuracy = 0.77797, accuracy/baseline_label_mean = 0.25339, accuracy/threshold_0.500000_mean = 0.77797, auc = 0.762874, auc_precision_recall = 0.540307, global_step = 20002, labels/actual_label_mean = 0.25339, labels/prediction_mean = 0.262542, loss = 0.478636, precision/positive_threshold_0.500000_mean = 0.640055, recall/positive_threshold_0.500000_mean = 0.282797\n",
      "evaluate done\n",
      "Accuracy: 0.77797\n",
      "{'accuracy/baseline_label_mean': 0.25339001, 'loss': 0.47863632, 'auc': 0.76287419, 'global_step': 20002, 'accuracy/threshold_0.500000_mean': 0.77797002, 'recall/positive_threshold_0.500000_mean': 0.28279728, 'labels/prediction_mean': 0.26254222, 'accuracy': 0.77797002, 'auc_precision_recall': 0.54030704, 'precision/positive_threshold_0.500000_mean': 0.640055, 'labels/actual_label_mean': 0.25339001}\n"
     ]
    }
   ],
   "source": [
    "eval_sample_size = 500000 # this can be found with a 'wc -l eval.csv'\n",
    "eval_steps = eval_sample_size/BATCH_SIZE\n",
    "\n",
    "results = m.evaluate(input_fn=generate_input_fn(eval_file), \n",
    "                     steps=eval_steps)\n",
    "print('evaluate done')\n",
    "\n",
    "print('Accuracy: %s' % results['accuracy'])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行预估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py:784: calling predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:143: setup_train_data_feeder (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:159: __init__ (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0e6e35ce9d25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wide and deep columns configured'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 553\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    555\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.pyc\u001b[0m in \u001b[0;36mpredict_classes\u001b[0;34m(self, x, input_fn, batch_size, as_iterable)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         as_iterable=as_iterable)\n\u001b[0m\u001b[1;32m    785\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mas_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_as_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, input_fn, batch_size, outputs, as_iterable, iterate_batches)\u001b[0m\n\u001b[1;32m    663\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mSKCompat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m     \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m     return self._infer_model(\n\u001b[1;32m    667\u001b[0m         \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36m_get_input_fn\u001b[0;34m(x, y, input_fn, feed_fn, batch_size, shuffle, epochs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m       \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m       epochs=epochs)\n\u001b[0m\u001b[1;32m    144\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_builder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feed_dict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m               instructions)\n\u001b[0;32m--> 306\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    308\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.pyc\u001b[0m in \u001b[0;36msetup_train_data_feeder\u001b[0;34m(x, y, n_classes, batch_size, shuffle, epochs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mStreamingDataFeeder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m   return data_feeder_cls(\n\u001b[0;32m--> 159\u001b[0;31m       x, y, n_classes, batch_size, shuffle=shuffle, epochs=epochs)\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m               instructions)\n\u001b[0;32m--> 306\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    308\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/py2tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, n_classes, batch_size, shuffle, random_state, epochs)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     self._x = dict([(k, check_array(v, v.dtype)) for k, v in list(x.items())\n\u001b[0;32m--> 340\u001b[0;31m                    ]) if x_is_dict else check_array(x, x.dtype)\n\u001b[0m\u001b[1;32m    341\u001b[0m     self._y = None if y is None else (dict(\n\u001b[1;32m    342\u001b[0m         [(k, check_array(v, v.dtype)) for k, v in list(y.items())])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "# def pred_input_fn():\n",
    "#     sample = [0, 127, 1, 3, 1683, 19, 26, 17, 475, 0, 9, 0, 3, \"05db9164\", \"8947f767\", \"11c9d79e\", \"52a787c8\", \"4cf72387\", \"fbad5c96\", \"18671b18\", \"0b153874\", \"a73ee510\", \"ceb10289\", \"77212bd7\", \"79507c6b\", \"7203f04e\", \"07d13a8f\", \"2c14c412\", \"49013ffe\", \"8efede7f\", \"bd17c3da\", \"f6a3e43b\", \"a458ea53\", \"35cd95c9\", \"ad3062eb\", \"c7dc6720\", \"3fdb382b\", \"010f6491\", \"49d68486\"]\n",
    "#     sample_dict = dict(zip(FEATURE_COLUMNS, sample))\n",
    "# #     print('Columns and data as a dict: ', sample_dict, '\\n')\n",
    "#     for feature_name in CATEGORICAL_COLUMNS:\n",
    "# #         print(sample_dict[feature_name])\n",
    "#         sample_dict[feature_name] = tf.expand_dims(sample_dict[feature_name], -1)\n",
    "\n",
    "#     for feature_name in CONTINUOUS_COLUMNS:\n",
    "#         sample_dict[feature_name] = tf.constant(sample_dict[feature_name], dtype=tf.int32)\n",
    "#     print(sample_dict)\n",
    "# #     print('Columns and data as a dict: ', sample_dict, '\\n')\n",
    "#     return sample_dict\n",
    "def pred_input_fn():\n",
    "    def _input_fn():\n",
    "        # 1个int型的label, 13个连续值, 26个字符串类型\n",
    "        cont_defaults = [ [0] for i in range(1,14) ]\n",
    "        cate_defaults = [ [\" \"] for i in range(1,27) ]\n",
    "        label_defaults = [ [0] ]\n",
    "        column_headers = TRAIN_DATA_COLUMNS\n",
    "        \n",
    "\n",
    "        \n",
    "        # 第一列数据是label\n",
    "        record_defaults = label_defaults + cont_defaults + cate_defaults\n",
    "\n",
    "        # 解析读出的csv数据\n",
    "        # 我们要手动把数据和header去zip在一起\n",
    "        sample = [0, 127, 1, 3, 1683, 19, 26, 17, 475, 0, 9, 0, 3, \"05db9164\", \"8947f767\", \"11c9d79e\", \"52a787c8\", \"4cf72387\", \"fbad5c96\", \"18671b18\", \"0b153874\", \"a73ee510\", \"ceb10289\", \"77212bd7\", \"79507c6b\", \"7203f04e\", \"07d13a8f\", \"2c14c412\", \"49013ffe\", \"8efede7f\", \"bd17c3da\", \"f6a3e43b\", \"a458ea53\", \"35cd95c9\", \"ad3062eb\", \"c7dc6720\", \"3fdb382b\", \"010f6491\", \"49d68486\"]\n",
    "        sample_dict = dict(zip(FEATURE_COLUMNS, sample))\n",
    "        \n",
    "        # 弹出和保存label标签\n",
    "#         labels = all_columns.pop(LABEL_COLUMN[0])\n",
    "        \n",
    "        # 其余列就是特征\n",
    "        features = sample_dict\n",
    "\n",
    "        return features\n",
    "    return _input_fn\n",
    "\n",
    "    wide_columns = []\n",
    "    for name in CATEGORICAL_COLUMNS:\n",
    "        wide_columns.append(tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "                name, hash_bucket_size=1000))\n",
    "    \n",
    "    deep_columns = []\n",
    "    for name in CONTINUOUS_COLUMNS:\n",
    "        deep_columns.append(tf.contrib.layers.real_valued_column(name))\n",
    "\n",
    "    print('deep/continuous columns configured')\n",
    "    for col in wide_columns:\n",
    "        deep_columns.append(tf.contrib.layers.embedding_column(col, dimension=8))\n",
    "    print('wide and deep columns configured')\n",
    "    \n",
    "result = m.predict_classes(x=pred_input_fn)\n",
    "print(result)\n",
    "for i in result:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py2tf]",
   "language": "python",
   "name": "conda-env-py2tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
